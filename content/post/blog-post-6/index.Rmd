---
title: 'Blog Post 6: Simulation'
author: Package Build
date: '2024-10-12'
slug: blog-post-6
categories: []
tags: []
---

```{r, echo=FALSE, warning=FALSE, message = FALSE}
# import dependencies
source("model_construction.R")

# read data
df <- read_csv("data/ec_merged_updated.csv")
df_econ <- read_csv("data/econ_fundamentals.csv")
df_polls <- read_csv("data/polls.csv")

# add new partisan lean variables for vote share analysis
df <- df %>% 
  mutate(
    pl_d_2pv = d_2pv - d_2pv_nat
  ) %>% 
  group_by(state) %>%       # Group by state
  arrange(year) %>%
  mutate(
    pl_d_2pv_lag1 = lag(pl_d_2pv),
    pl_d_2pv_lag2 = lag(pl_d_2pv_lag1)
    ) %>% 
  relocate(pl_d_2pv, pl_d_2pv_lag1, pl_d_2pv_lag2, .after = pl_lag2) 

# add new polling variables for vote share analysis
# add national polling variable
df_polls_nat_pivoted <- df_polls %>%
  # collapse to get national dataset
  group_by(year, weeks_left) %>% 
  summarize(
    d_poll_nat = first(d_poll_nat)
  ) %>% 
  pivot_wider(
    names_from = c("weeks_left"),
    values_from = c("d_poll_nat"),
    names_prefix = "poll_pv_nat_"
  )

# add state polling variable
df_polls_state_pivoted <- df_polls %>%
  dplyr::select(year, state, weeks_left, d_poll_state) %>%
  pivot_wider(
    names_from = c("weeks_left"),
    values_from = c("d_poll_state"),
    names_prefix = "poll_pv_state_"
  )

# Create a vector of column names for poll_margin and poll_margin_nat
pv_state_cols <- paste0("poll_pv_state_", 1:36)
pv_nat_cols <- paste0("poll_pv_nat_", 1:36)

# merge polling fix into full dataset
df <- df %>% 
  left_join(df_polls_nat_pivoted, by = "year") %>% 
  left_join(df_polls_state_pivoted, by = c("year", "state"))

# Create new columns for poll_lean
for (i in 1:36) {
  new_col_name <- paste0("poll_pv_lean_", i)
  df[[new_col_name]] <- df[[pv_state_cols[i]]] - df[[pv_nat_cols[i]]]
}

# fix elasticity computation
df <- df %>%
  group_by(state) %>% 
  mutate(
    elasticity = ifelse(
      year == 1952, 
      1, 
      abs(((d_2pv - lag(d_2pv)) / lag(d_2pv)) / ((d_2pv_nat - lag(d_2pv_nat)) / lag(d_2pv_nat)))
    ),
    elasticity_lag1 = lag(elasticity),
    elasticity_lag2 = lag(elasticity_lag1)
  )

write_csv(df, "data/ec_merged_updated_real.csv")
```

```{r, echo=FALSE, warning=FALSE, message = FALSE}
df <- read_csv("data/ec_merged_updated_real.csv")

indicators <- c("jobs", "pce", "rdpi", "cpi", "ics", "sp500", "unemp")
period_start <- -30
period_end <- -8 #if want 2024, can't set this super close

df <- aggregate_indicators(df, df_econ, indicators, period_start, period_end, rate_aggregation)


# create dataframe subsets
df_subset_1972 <- df %>% filter(year >= 1972)
df_subset_1980 <- df %>% filter(year >= 1980)
df_subset_2000 <- df %>% filter(year >= 2000) 
```


```{r, echo=FALSE, warning=FALSE, message = FALSE}
# define formulas
state_formula <- as.formula(paste("pl_d_2pv ~ pl_d_2pv_lag1 + pl_d_2pv_lag2 + hsa_adjustment +",
                                  "rsa_adjustment + elasticity +", 
                                  "cpr_solid_d + cpr_likely_d	+ cpr_lean_d +", 
                                  "cpr_toss_up + cpr_lean_r + cpr_likely_r	+ cpr_solid_r + ",
                                  paste0("poll_pv_lean_", 7:36, collapse = " + ")))

nat_fund_formula <- as.formula("d_2pv_nat ~ incumb_party:(jobs_agg + 
                                     pce_agg + rdpi_agg + cpi_agg + ics_agg + 
                                     sp500_agg + unemp_agg)")


nat_polls_formula <- as.formula(paste("d_2pv_nat ~ incumb_party:(weighted_avg_approval) + ", 
                                           paste0("poll_pv_nat_", 7:36, collapse = " + ")))

# Split data, using the 1972 subset
state_data <- split_state(df_subset_1972, 2024)
national_data <- split_national(df_subset_1972, 2024)

# Predict state elasticities
state_data$test <- predict_elasticity(state_data$train, state_data$test)

# Train models
state_model_info <- train_elastic_net_fast(state_data$train, state_formula)
nat_fund_model_info <- train_elastic_net_fast(national_data$train, nat_fund_formula)
nat_polls_model_info <- train_elastic_net_fast(national_data$train, nat_polls_formula)
ensemble <- train_ensemble(list(nat_fund_model_info, nat_polls_model_info))

# Make predictions
state_predictions <- make_prediction(state_model_info, state_data$test)
nat_fund_predictions <- make_prediction(nat_fund_model_info, national_data$test)
nat_polls_predictions <- make_prediction(nat_polls_model_info, national_data$test)
# ensemble_predictions <- make_ensemble_prediction(ensemble, national_data$test)

# Create the prediction tibble
df_2024 <- tibble(
  state = state_data$test$state,
  abbr = state_data$test$abbr,
  electors = state_data$test$electors,
  partisan_lean = as.vector(state_predictions)
  ) %>%
  # Add national predictions - using first value since they're the same for all states
  mutate(
    d_2pv_polls = first(as.vector(nat_polls_predictions)),
    d_2pv_fund = first(as.vector(nat_fund_predictions))
 #   d_2pv_ensemble = first(as.vector(ensemble_predictions))
  ) %>%
  # Calculate final margins and color categories
  mutate(
    d_2pv_final = partisan_lean + d_2pv_polls,
    d_pv = d_2pv_final,
    r_pv = 100 - d_2pv_final,
    category = case_when(
      d_pv > 60 ~ "Strong D",
      d_pv > 55 & d_pv < 60 ~ "Likely D",
      d_pv > 50 & d_pv < 55 ~ "Lean D",
      d_pv > 45 & d_pv < 50 ~ "Lean R",
      d_pv > 40 & d_pv < 45 ~ "Likely R",
      TRUE ~ "Strong R"
    ),
    # Convert to factor with specific ordering
    category = factor(
      category,
      levels = c("Strong R", "Likely R", "Lean R", "Lean D", "Likely D", "Strong D")
    ),
    # calculate electors that each party wins
    d_electors = sum(ifelse(category %in% c("Lean D", "Likely D", "Strong D"), electors, 0)),
    r_electors = sum(ifelse(category %in% c("Lean R", "Likely R", "Strong R"), electors, 0))
  ) %>% 
  # filter unnecessary districts
  filter(!abbr %in% c("ME_d1", "NE_d1", "NE_d3"))

# run simulation
election <- simulate_election(nat_polls_model_info, state_model_info, national_data$test, state_data$test)


write_csv(df_2024, "df_2024.csv")
```

# Overview
Okay, something is seriously wrong with my model. Recall that my final model's predictions involve an ensemble between the fundamentals model and the polling model. Well, this week when I ran the model, I realized that the ensemble assigned a _negative_ weight to the fundamentals model. Admittedly it was only slightly negative, but still --- that shouldn't happen. Immediately, alarm bells went off. First of all, I modified the ensemble model from performing ordinary least squares, your classic regression, to non-negative least squares, which imposes an added restriction that all coefficients must be non-negative. I solved the optimization problem using [this](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1099-128X(199709/10)11:5%3C393::AID-CEM483%3E3.0.CO;2-L) method. Still, though, the ensemble model shrank the fundamentals model to zero, and assigned the polls model a weight of $1$.

Odd.

I confess, I spent a lot of time trying to find out why the fundamentals model was so bad, and I couldn't entirely figure it out. I tried multiple different permutations of the regression specification, I tried aggregating the economic fundamentals a different way, I tried adding the base terms to the regression in addition to the cross terms. Nothing worked. Consistently, I would get $R^2$ as low as $0.3$, and an adjusted $R^2$ in the negatives. The out-of-sample MSE was often over four times as large as the in-sample MSE. And worst of all, the expected sign on the coefficients were the wrong direction --- somehow, real disposable personal income was a negative predictor of vote margin, not a positive one.

Perhaps I will figure out what's up with the fundamentals in a future week. In the mean time, it doesn't actually affect my predictions that much --- the polls model seems to be quite accurate:
- $R^2 = 0.879$
- $R_{\mathrm{adj}}^{2} = 0.793$
- $MSE_{\mathrm{out}} = 4.8$

One nice thing about this bug is that it actually inspired me to rerun the entire model, except instead of vote margin, I used vote share. While it didn't solve the issue with the fundamentals model, it did reveal something: the vote share model is actually more accurate than the vote margin one! Moving forward, I will be using the vote share model instead. (This also has the added benefit of inadvertently resolving my Washington D.C. vote share issue from last week.)

I also fixed two other small mistakes. First, I realized that I was calculating a state's elasticity incorrectly. Rather than measuring it in absolute terms, I changed it to be in relative terms:

$$
\varepsilon_t = \frac{\frac{s_{t} - s_{t-1}}{s_{t-1}}}{\frac{n_{t} - n_{t-1}}{n_{t-1}}}
$$
where $s_{t}$ is the state's vote in year $t$ and $n_{t}$ is the national vote in year $t$.

Second, I noticed that my state level forecast for the partisan lead included significant coefficients for several of Cook Political Report forecasts metics. However, I had neglected to include these forecasts in the testing data, so my results were somewhat biased. All these changes came together to give me the following map of the 2024 election:
```{r, echo=FALSE, warning=FALSE, message = FALSE}
electoral_map <- create_electoral_hex_map(df_2024)

electoral_map
```

A much better result for Harris, don't you think?


# A New Simulation Method
My biggest improvement this week, however, was in the simulation process.

Simulations are used to test the uncertainty in an election prediction model. Running simulations allows me to introduce a slight variations in the values of my estimates and see just how much the election outcome changes as a result of these variations. Last week, I implemented an incredibly crude simulation approach. I assumed that the predicted national vote share was absolutely true, and that the predicted vote at the state level was the mean of a normal random variable with a standard deviation of three points.

Both of these assumptions are questionable. My prediction for the national vote share also has some degree of uncertainty, of course. Not to mention the fact that a three-point standard deviation was chosen completely arbitrarily. This week, I will introduce a new  simulation process so that I avoid having to make the same assumptions I did last week.

For this simulation approach, I will introduce uncertainty in my _estimates_, not my _predictions_. This is an important distinction: the estimates refer to the vector of coefficients, $\vec{\beta}$, that I calculated using multiple regression. The predictions, on the other hand, refer to the vector of outputs, $\vec{\hat{y}}$, that I calculated by plugging in the 2024 values for all my predictors and computing a linear combination. I argue that varying $\vec{\beta}$ makes more sense than varying $\vec{\hat{y}}$, because it accounts for the fact that sampling variability makes the model's estimates of the underlying relationships between the different variables uncertain. The coefficients could be slightly different if I had different data, so by introducing variation in $\vec{\beta}$, I am capturing the true uncertainty in how predictors relate to the outcome.

So, I now simulate a set of coefficients by drawing from a multivariate normal distribution where the mean of each coefficient is the point estimate from the model, and the variance of each coefficient is determined by the square of its standard error. Of course, unlike with OLS regression, there is no easy closed form to get the variance of each coefficient. Instead, we have to "bootstrap" our model to calculate standard errors. Bootstrapping is a process where I draw $n$ observations from my original dataset _with replacement_ $s$ different times. I will then make $s$ different "copies" of my data. Then, I refit the model on each sample, and observe how the coefficients vary. By doing this, I can empirically estimate the variability in my coefficient estimates, and thus, the uncertainty in my election predictions.

Without further ado, here is the distribution:

```{r, echo=FALSE, warning=FALSE, message = FALSE}
election %>% 
  ggplot() +
    # Smoothed density for Democratic wins
    geom_density(aes(x = d_ev), fill = "lightblue", color = "blue", alpha = 0.3, adjust = 1.5) +
    # Smoothed density for Republican wins
    geom_density(aes(x = r_ev), fill = "lightcoral", color = "red", alpha = 0.3, adjust = 1.5) +
    # Vertical line at 270 electoral votes
    geom_vline(xintercept = 270, linetype = "dashed", color = "black", size = 1) +
    # Labels and title
    labs(
      title = "Monte Carlo Simulation Results: Electoral Vote Distribution",
      x = "Electoral Votes",
      y = "Density"
    ) +
    # Clean theme for better visualization
    theme_minimal(base_size = 15) +
    theme(legend.position = "none")

```


Woah. Something is definitely wrong. First of all, it is good that the Republican and Democratic electoral college votes are symmetric. But the mode of the distribution should not be all the way in the 400s. After all, my point estimate indicated that Harris wins just 287 votes --- enough to win, but not a landslide. Second, the distribution is quite wonky and uninterpretable. This is a problem, considering that it makes intuitive sense for closer elections to be more likely, and distant elections to be less so.

Ultimately, beyond coding mistakes (of which I am sure there are a few), I can think of several problems with my approach. First, my code, as written, introduces variation even in coefficients that the elastic net has shrunk to zero. These don't bias my estimates, because the normal random variable would just be centered around zero, but they do introduce extranous noise into my simulation. Second, I am varying many coefficients at once. And not all these coefficients are statistically significant, which means they sometimes have a pretty high standard error. As a result, we see large swings in electoral college results, because the slight variances all get magnified. A slightly larger $\beta_1$ gets applied, across the board, to all 50 states (and a few districts), which can end up having huge effects.

It seems that my prior was wrong, and that, in fact, it might be better to introduce variance somewhere else in the model. Next week, I will attempt to introduce variance in the testing data.
