---
title: 'Blog Post 5: Quantifying Uncertainty'
author: Package Build
date: '2024-10-06'
slug: blog-post-5
categories: []
tags: []
---

```{r, echo=FALSE, warning=FALSE, message = FALSE}
# Load libraries.
## install via `install.packages("name")`
library(ggplot2)
library(maps)
library(tidyverse)
library(usmap)
library(plotly)
library(gridExtra)
library(sf)
library(ggrepel)
library(shiny)
library(leaflet)
library(stargazer)
library(blogdown)
library(car)
library(lubridate)
library(zoo)
library(modelsummary)
library(GGally)
library(readxl)
library(caret)
library(CVXR)
library(glmnet)
library(stats)
library(formula.tools)
```

```{r, echo=FALSE, warning=FALSE, message = FALSE}
df <- read_csv("data/ec_merged_updated.csv")
df_econ <- read_csv("data/econ_fundamentals.csv")
```

```{r, echo=FALSE, warning=FALSE, message = FALSE}
# Aggregate economic indicators

rate_aggregation <- function(data, indicator, t1, t2) {
  subset <- data %>%
    filter(month_index >= t1 & month_index <= t2) %>%
    group_by(election_cycle) %>% 
    summarize(
      year = mean(election_year),
      value_t1 = first(.data[[indicator]][month_index == t1]),
      value_t2 = first(.data[[indicator]][month_index == t2]),
      # Dynamic naming of the aggregated variable
      !!paste0(indicator, "_agg") := (value_t2 - value_t1) / value_t1 * 100
    )
  return(subset %>% select(-value_t1, -value_t2, -election_cycle))
}
aggregate_indicators <- function(df_full, df_fundamentals, indicators, period_start, period_end, aggregation_method) {
  agg_results <- map(indicators, ~aggregation_method(df_fundamentals, .x, period_start, period_end))
  combined_agg <- reduce(agg_results, full_join, by = "year") %>% 
    mutate(across(ends_with("_agg"), ~as.numeric(scale(.))))
  df_full_merged <- df_full %>%
    left_join(combined_agg, by = "year") %>%
  return(df_full_merged)
}

# merge economic indicators
indicators <- c("jobs", "pce", "rdpi", "cpi", "ics", "sp500", "unemp")
period_start <- -30
period_end <- -8 #if want 2024, can't set this super close

df <- aggregate_indicators(df, df_econ, indicators, period_start, period_end, rate_aggregation)
```

```{r, echo=FALSE, warning=FALSE, message = FALSE}
# Function to split state-level prediction data
split_state <- function(df, y) {
  subset <- df %>% 
    select(
      year,
      state,
      abbr,
      electors,
      pl,
      pl_lag1, 
      pl_lag2, 
      hsa_adjustment, 
      rsa_adjustment, 
      elasticity, 
      all_of(starts_with("cpr_")),
      all_of(paste0("poll_lean_", 7:36))
    )
  train_data <- subset %>% filter(year < y)
  test_data <- subset %>% filter(year == y)
  return(list(train = train_data, test = test_data))
}

# Function to split national-level prediction data
split_national <- function(df, y) {
  subset <- df %>% 
    select(-c(all_of(paste0("poll_margin_nat_", 0:6)))) %>% 
    group_by(year) %>% 
      summarize(
        margin_nat = first(margin_nat),
        jobs_agg = first(jobs_agg), 
        pce_agg = first(pce_agg), 
        rdpi_agg = first(rdpi_agg), 
        cpi_agg = first(cpi_agg), 
        ics_agg = first(ics_agg), 
        sp500_agg = first(sp500_agg), 
        unemp_agg = first(unemp_agg),
        weighted_avg_approval = first(weighted_avg_approval),
        incumb = first(incumb),
        incumb_party = first(incumb_party),
        across(starts_with("poll_margin_nat_"), ~ mean(.x, na.rm = TRUE))
      )
  
  train_data <- subset %>% filter(year < y)
  test_data <- subset %>% filter(year == y)
  return(list(train = train_data, test = test_data))
}

train_elastic_net <- function(df, formula) {
  
  # Create matrix of MSE for every alpha, lambda, year left out during cross validation
  alpha_range <- seq(0, 1, length.out = 11)
  lambda_range <- seq(0, 5, length.out = 101)
  years <- unique(df %>% pull(year))
  mse_matrix <- expand_grid(year = years, alpha = alpha_range, lambda = lambda_range) %>% 
    # create column for MSE, set to NA
    mutate(mse = NA_real_)
  
  count <- 1
  # iterate over the years
  for (year in years) {
    
    # Create validation data by excluding all rows from the current year
    train_data <- df %>% filter(year != !!year)
    val_data <- df %>% filter(year == !!year)

    # Create training model matrix from formula and training data
    mf_train <- model.frame(formula, data = train_data, na.action = na.pass) # keep NA values
    X_train <- model.matrix(formula, mf_train)[, -1]  # Remove intercept column
    y_train <- model.response(mf_train)

        # Create validation model matrix from formula and validation data
    mf_val <- model.frame(formula, data = val_data, na.action = na.pass) # keep columns with NA values
    X_val <- model.matrix(formula, mf_val)[, -1]  # Remove intercept column
    y_val <- model.response(mf_val)
    
    # Handle missing predictors
    X_train[is.na(X_train)] <- 0  # Replace NA with 0
    X_val[is.na(X_val)] <- 0
    
    # Iterate over alpha and lambda values
    for (alpha in alpha_range) {
      for (lambda in lambda_range) {
        
        # train the model at the given alpha and lambda levels
        model <- glmnet(X_train, y_train, alpha = alpha, lambda = lambda)

        # compute out of sample MSE on validation data
        predictions_val <- predict(model, newx = X_val)
        mse_val <- mean((predictions_val - y_val)^2, na.rm = TRUE)
        
        # store in corresponding slot of MSE matrix
        mse_matrix$mse[mse_matrix$year == year & 
                     mse_matrix$alpha == alpha & 
                     mse_matrix$lambda == lambda] <- mse_val
        
        #print(paste("Results: Iteration #", count))
        #print(paste("MSE: ", mse_val, ". alpha: ", alpha, ". lambda: ", lambda))
        #count <- count + 1
      }
    }
  }
  
  # calculate the minimum average MSE for each alpha and lambda
  avg_mse <- mse_matrix %>%
    group_by(alpha, lambda) %>%
    summarize(avg_mse = mean(mse, na.rm = TRUE), .groups = "drop")
  
  best_params <- avg_mse %>%
    filter(avg_mse == min(avg_mse)) %>%
    slice(1)
  
  best_alpha <- best_params$alpha
  best_lambda <- best_params$lambda
  min_avg_mse <- best_params$avg_mse
  
  # create full model matrix
  mf <- model.frame(formula, data = df, na.action = na.pass) # keep NA values
  X <- model.matrix(formula, mf)[, -1]  # Remove intercept column
  y <- model.response(mf)

  # Handle missing predictors
  X[is.na(X)] <- 0  # Replace NA with 0
  
  # train on full data
  model <- glmnet(X, y, alpha = best_alpha, lambda = best_lambda)
  
  # Calculate adjusted R-squared
  y_pred <- predict(model, newx = X)
  n <- nrow(X)
  p <- sum(coef(model) != 0) - 1  # number of non-zero coefficients (excluding intercept)
  tss <- sum((y - mean(y))^2)
  rss <- sum((y - y_pred)^2)
  r_squared <- 1 - (rss / tss)
  adj_r_squared <- 1 - ((1 - r_squared) * (n - 1) / (n - p - 1))
  
  mse_plot <- avg_mse %>%
    ggplot(aes(x = alpha, y = lambda, fill = avg_mse)) +
    geom_tile() +
    scale_fill_viridis_c(trans = "log10") +
    labs(title = "Average MSE for different alpha and lambda values",
         x = "Alpha", y = "Lambda", fill = "Average MSE") +
    theme_minimal() +
    geom_point(data = best_params, aes(x = alpha, y = lambda), 
               color = "red", size = 3, shape = 4)
  
  # return the model, optimal alpha and lamba, R^2 and other in-sample errors, and out-of-sample MSE 
  list(
    model = model,
    formula = formula,
    alpha = best_alpha,
    lambda = best_lambda,
    y_pred = as.vector(y_pred),
    y_actual = as.vector(y),
    n = n,
    p = p,
    tss = tss,
    rss = rss,
    r_squared = r_squared,
    adj_r_squared = adj_r_squared,
    out_of_sample_mse = min_avg_mse,
    in_sample_mse = rss / n,
    mse_plot = mse_plot
  )
}

# get model coefficients
get_coef <- function(model_info) {
  return(as.vector(coef(model_info$model))) 
}

make_prediction <- function(model_info, new_data) {
  # Handle missing or NaN values
  new_data[is.na(new_data) | is.nan(as.matrix(new_data))] <- 0
  
  # Create the model matrix using the stored formula
  X_new <- model.matrix(model_info$formula, data = new_data, na.action = na.pass)[, -1]
  
  # Predict using the glmnet model
  return(predict(model_info$model, newx = X_new, s = model_info$lambda))
}

# Stack models to create an ensemble prediction
train_ensemble <- function(model_infos) {
  
  # extract in-sample predictions
  in_sample_predictions <- lapply(model_infos, function(model_info) model_info$y_pred)
  
  # combine predictions into a new dataset
  meta_features <- do.call(cbind, in_sample_predictions) %>% as.data.frame()
  colnames(meta_features) <- paste0("model_", seq_along(model_infos))
  
  # train the meta-model
  y <- model_infos[[1]]$y_actual
  meta_model <- lm(y ~ . - 1, data = meta_features) # train on all columns
  
  # extract and normalize coefficients
  coef <- coef(meta_model)
  normalized_coef <- coef / sum(coef)
  
  # calculate y_pred using normalized coefficients
  y_pred <- as.matrix(meta_features) %*% normalized_coef
  
  # calculate r_squared and adjusted r_squared
  n <- length(y)
  p <- length(coef(meta_model)) - 1  # number of predictors (excluding intercept)
  tss <- sum((y - mean(y))^2)
  rss <- sum((y - y_pred)^2)
  r_squared <- 1 - (rss / tss)
  adj_r_squared <- 1 - ((1 - r_squared) * (n - 1) / (n - p - 1))
  
  # Return the meta-model and associated statistics
  list(
    model = meta_model,
    sub_models = model_infos,
    y_pred = as.vector(y_pred),
    y_actual = as.vector(y),
    weights = normalized_coef,
    n = n,
    p = p,
    tss = tss,
    rss = rss,
    r_squared = r_squared,
    adj_r_squared = adj_r_squared
  )
}

# Function to calculate overfit
calculate_overfit <- function(MSE_out, MSE_in, n, p) {
  overfit <- MSE_out - ((n + p + 1) / (n - p - 1)) * MSE_in
  return(overfit)
}

# function to make prediction for ensemble model
make_ensemble_prediction <- function(ensemble_model_info, new_data) {
  # Make predictions using each sub-model
  predictions <- lapply(ensemble_model_info$sub_models, 
                        function(model_info) make_prediction(model_info, new_data))
  
  # Combine predictions into a data frame
  meta_features <- do.call(cbind, predictions) %>% as.data.frame()
  colnames(meta_features) <- paste0("model_", seq_along(predictions))
  
  # Use the meta-model to make the final prediction
  final_prediction <- predict(ensemble_model_info$model, newdata = meta_features)
  
  return(final_prediction)
}

# Define the color palette
map_colors <- tibble(
  category = c("Strong R", "Likely R", "Lean R", "Lean D", "Likely D", "Strong D"),
  colors = c("#e48782", "#f0bbb8", "#fbeeed", "#e5f3fd", "#6ac5fe", "#0276ab")
) %>%
  deframe()

# Join the hex coordinates with our prediction data
create_electoral_hex_map <- function(prediction_data) {
  # Define hex coordinates using proper hexagonal grid spacing
  # In a hex grid, horizontal spacing is 1 unit, vertical is 0.866 units (sin(60°))
  hex_coords <- tibble(
    abbr = c(
      # Row 1 (top)
      "AK",                                                              "ME",
      # Row 2
      "WA",                                                    "VT", "NH", "ME_d2",
      # Row 3
      "OR", "MT", "ND", "MN", "WI",      "MI",              "NY", "MA", "RI",
      # Row 4
      "CA", "ID", "WY", "SD", "IA", "IL", "IN", "OH", "PA", "NJ", "CT",
      # Row 5
      "NV", "CO", "NE", "NE_d2", "MO", "KY", "WV", "VA", "MD", "DE",
      # Row 6
      "AZ", "UT", "NM", "KS", "AR", "TN", "NC", "SC", "DC",
      # Row 7
           "OK", "LA", "MS", "AL", "GA",
      # Row 8
      "HI",      "TX",                    "FL"
    ),
    x = c(
      # Row 1
      1,                                            11,
      # Row 2
      1,                                   9, 10, 11,
      # Row 3
      1, 2, 3, 4, 5,              7,     9, 10, 11,
      # Row 4
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
      # Row 5
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
      # Row 6
      1, 2, 3, 4, 5, 6, 7, 8, 9,
      # Row 7
           3, 4, 5, 6, 7,
      # Row 8
      0,       3,                     8
    ),
    y = c(
      # Row 1
      9,                                           9,
      # Row 2
      8,                                    8, 8, 8,
      # Row 3
      7, 7, 7, 7, 7,    7,              7, 7, 7,
      # Row 4
      6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
      # Row 5
      5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
      # Row 6
      4, 4, 4, 4, 4, 4, 4, 4, 4,
      # Row 7
           3, 3, 3, 3, 3,
      # Row 8
      2,       2,                     2
    )
  )
  
  # Calculate proper hexagonal spacing
  hex_size <- 40  # Base size of hexagons
  hex_width <- hex_size * 2
  hex_height <- hex_size * sqrt(3)
  
  # Apply hexagonal grid transformations
  hex_coords <- hex_coords %>%
    mutate(
      # Offset every other row horizontally by half a hex width
      x = case_when(
        y %% 2 == 0 ~ x * hex_width,
        TRUE ~ x * hex_width + hex_width/2
      ),
      # Scale y coordinates to create proper hexagonal spacing
      y = y * hex_height * 0.75  # 0.75 factor for tighter vertical spacing
    )
  
  map_data <- prediction_data %>%
    left_join(hex_coords, by = "abbr") %>%
    mutate(
      hover_text = sprintf(
        "%s\nVotes: %d\nDem: %.1f%%\nRep: %.1f%%",
        abbr, 
        electors,
        d_pv,
        r_pv
      )
    )

  hex_map <- plot_ly(
    data = map_data,
    type = "scatter",
    mode = "markers",
    x = ~x,
    y = ~y,
    color = ~category,
    colors = map_colors,
    marker = list(
      symbol = "hexagon",
      size = hex_size,
      line = list(color = "white", width = 1)
    ),
    text = ~hover_text,
    hoverinfo = "text"
  ) %>%
    layout(
      title = list(
        text = "2024 Electoral College Prediction",
        x = 0.5,
        y = 0.95
      ),
      showlegend = TRUE,
      xaxis = list(
        showgrid = FALSE,
        zeroline = FALSE,
        showticklabels = FALSE,
        range = c(-50, hex_width * 12),
        title = ""
      ),
      yaxis = list(
        showgrid = FALSE,
        zeroline = FALSE,
        showticklabels = FALSE,
        range = c(0, hex_height * 10),
        scaleanchor = "x",  # This ensures hexagons remain regular
        scaleratio = 1,
        title = ""
      ),
      plot_bgcolor = "white",
      paper_bgcolor = "white"
    )

  # Add state labels
  hex_map <- hex_map %>%
    add_annotations(
      data = map_data,
      x = ~x,
      y = ~y,
      text = ~abbr,
      showarrow = FALSE,
      font = list(size = 12, color = "black")
    )

  # Add electoral vote totals
  subtitle <- sprintf(
    "Democratic EVs: %d | Republican EVs: %d",
    unique(map_data$d_electors),
    unique(map_data$r_electors)
  )

  hex_map <- hex_map %>%
    layout(
      annotations = list(
        list(
          x = 0.5,
          y = -0.1,
          text = subtitle,
          showarrow = FALSE,
          xref = "paper",
          yref = "paper",
          font = list(size = 14)
        )
      )
    )

  return(hex_map)
}
```


Last week, I finally completed the full extent of my model architecture! We have now incorporated most of the data that we will use in our final model, multiple layers of prediction using an elastic net regression with optimized parameters, and model ensembling. Three items are on our agenda for this week. First, I will fix a few lingering errors that I noticed in last week's model. Then, I will build a visualization for my 2024 election predictions. And finally, I will attempt to implement a Monte Carlo simulation that tests the uncertainty of my model.

# Error Correction
I noticed two errors in my previous model. First, last week, I incorrectly assumed that the ensemble learning weights do not have to sum to one. Turns out, I misinterpreted super learning as an unconstrained optimization problem. In fact, the idea is to take a convex combination of each base model and produce a single prediction, which means that the weights actually do have to add to one. In this week's model, I add that constraint.

Second, I may need to modify my method for imputing missing data. Currently, there are many indicators --- both polls and economic indicators --- that do not extend as far back in time as my output variable. However, in previous models, I thought it would be remiss to forego a few extra years of training simply because a few indicators are missing. In fact, there is so much data missing, that if I threw out every state or every year in which an indicator was missing, I would end up with hardly any training data at all! Because there is so much missing data, it is also not impossible to impute. After all, I can't just invent polling data for an entire year!

So, what I have done in previous weeks is to simply impute the missing indicators as zero. That way, when I train the model, the coefficients on the missing indicators will not effect the coefficients of the non-missing indicators. However, this approach, while plausible, could potentially bias the coefficients of both indicators themselves. (Here, when I say "missing" indicators, I am referring indicators for which data during some subset of the years from 1952 to 2022 are missing.) For example, suppose poll_margin_nat_7 --- the indicator for the national polling margin seven weeks before the election --- is missing from the years 1952 to 2000, and accurate from 2000 to 2020. Then, the coefficient of the polling variable is likely biased downward because, for the earlier period, I have assumed that polling had no effect on the vote margin (when in reality, it likely did). Similarly, the other variables are likely biased upwards, because they could be picking up some of the variation that should have been explained by polling.

Unfortunately, this issue isn't easy to solve. I can minimize the bias by excluding years with lots of missing indicators from my dataset, but that reduces that already-small sample I have to train on, which could cause overfitting and make both my in-sample and out-of-sample predictions less accurate. To be rigorous about this, let's define a precise "overfitting" metric as the differential between the out-of-sample mean squared error of a given model and some function of the in-sample mean squared error of that model. 

If the model is correctly specified, it can be shown under mild assumptions that the expected value of the MSE for the training set (i.e. our in-sample MSE) is (n − p − 1)/(n + p + 1) < 1 times the expected value of the MSE for the validation set (i.e. our out-of-sample MSE), where n is the number of observations, and p is the number of features. Luckily,it is possible to directly compute the factor (n − p − 1)/(n + p + 1) by which the training MSE underestimates the validation MSE. So, we can create our "overfitted" metric as:
$$
\mathrm{overfit} = \mathrm{MSE\_out} - \left(\frac{n + p + 1}{n − p − 1}\right) \mathrm{MSE\_in}
$$

The following table reports the overfitting metric for each of three potential subsets of the data:

```{r}
# Define formulas
state_formula <- as.formula(paste("pl ~ pl_lag1 + pl_lag2 + hsa_adjustment +",
                                  "rsa_adjustment + elasticity +", 
                                  "cpr_solid_d + cpr_likely_d	+ cpr_lean_d +", 
                                  "cpr_toss_up + cpr_lean_r + cpr_likely_r	+ cpr_solid_r + ",
                                  paste0("poll_lean_", 7:36, collapse = " + ")))

nat_fund_formula <- as.formula("margin_nat ~ incumb_party:(jobs_agg + 
                                     pce_agg + rdpi_agg + cpi_agg + ics_agg + 
                                     sp500_agg + unemp_agg)")

nat_polls_formula <- as.formula(paste("margin_nat ~ incumb_party:(weighted_avg_approval) + ", 
                                           paste0("poll_margin_nat_", 7:36, collapse = " + ")))

# create lists of dataframes for comparison
df_subset_1972 <- df %>% filter(year >= 1972)
df_subset_1980 <- df %>% filter(year >= 1980)
df_subset_2000 <- df %>% filter(year >= 2000) 
dfs <-  list(df, df_subset_1972, df_subset_1980, df_subset_2000)

# Initialize a matrix to store the MSEs (3 models x 4 subsets)
mse_matrix <- matrix(nrow = 4, ncol = 3)
rownames(mse_matrix) <- c("Full Data", "Subset >= 1972", "Subset >= 1980", "Subset >= 2000")
colnames(mse_matrix) <- c("State Model", "Nat Fund Model", "Nat Polls Model")

for (i in seq_along(dfs)) {
  # access df
  df <- dfs[[i]]
  
  # Split data
  state_data <- split_state(df, 2024)
  national_data <- split_national(df, 2024)
  
  # Train models
  state_model_info <- train_elastic_net(state_data$train, state_formula)
  nat_fund_model_info <- train_elastic_net(national_data$train, nat_fund_formula)
  nat_polls_model_info <- train_elastic_net(national_data$train, nat_polls_formula)

  # Store MSEs in the matrix
  mse_matrix[i, 1] <- calculate_overfit(state_model_info$out_of_sample_mse, state_model_info$in_sample_mse,
                                        state_model_info$n, state_model_info$p)
  mse_matrix[i, 2] <- calculate_overfit(nat_fund_model_info$out_of_sample_mse, nat_fund_model_info$in_sample_mse,
                                        nat_fund_model_info$n, nat_fund_model_info$p)
  mse_matrix[i, 3] <- calculate_overfit(nat_polls_model_info$out_of_sample_mse, nat_polls_model_info$in_sample_mse,
                                        nat_polls_model_info$n, nat_polls_model_info$p)
}

print(mse_matrix)
```
This testing suggests that the 1972 subset is best, which is what I will use for the remainder of the blog.

# Visualizing the prediction
Voila! Below, find my predicted electoral map for the 2024 election. I used an interactive hex map, visualized with plotly, using the standard coordinates for the US states and districts. There are still some issues with this prediction. For one, the prediction for Washington D.C. says that the Democrats will win approximately 104 percent of the vote share. Now, I'm from D.C. --- and trust me: we vote very blue. But I know for a fact that there's no way the Democrats win 104 percent of our residents! Something is clearly wrong, likely because my outcome variable is not bounded. This will be fixed in future blog iterations.

```{r}

# Define formulas
state_formula <- as.formula(paste("pl ~ pl_lag1 + pl_lag2 + hsa_adjustment +",
                                  "rsa_adjustment + elasticity +", 
                                  "cpr_solid_d + cpr_likely_d	+ cpr_lean_d +", 
                                  "cpr_toss_up + cpr_lean_r + cpr_likely_r	+ cpr_solid_r + ",
                                  paste0("poll_lean_", 7:36, collapse = " + ")))

nat_fund_formula <- as.formula("margin_nat ~ incumb_party:(jobs_agg + 
                                     pce_agg + rdpi_agg + cpi_agg + ics_agg + 
                                     sp500_agg + unemp_agg)")

nat_polls_formula <- as.formula(paste("margin_nat ~ incumb_party:(weighted_avg_approval) + ", 
                                           paste0("poll_margin_nat_", 7:36, collapse = " + ")))

# Split data, using the 1972 subset
state_data <- split_state(df_subset_1972, 2024)
national_data <- split_national(df_subset_1972, 2024)

# Train models
state_model_info <- train_elastic_net(state_data$train, state_formula)
nat_fund_model_info <- train_elastic_net(national_data$train, nat_fund_formula)
nat_polls_model_info <- train_elastic_net(national_data$train, nat_polls_formula)
ensemble <- train_ensemble(list(nat_fund_model_info, nat_polls_model_info))

# Make predictions
state_predictions <- make_prediction(state_model_info, state_data$test)
nat_fund_predictions <- make_prediction(nat_fund_model_info, national_data$test)
nat_polls_predictions <- make_prediction(nat_polls_model_info, national_data$test)
ensemble_predictions <- make_ensemble_prediction(ensemble, national_data$test)

# Create the prediction tibble
df_2024 <- tibble(
  state = state_data$test$state,
  abbr = state_data$test$abbr,
  electors = state_data$test$electors,
  partisan_lean = as.vector(state_predictions)
  ) %>%
  # filter unnecessary districts
  filter(!abbr %in% c("ME_d1", "NE_d1", "NE_d3")) %>% 
  # Add national predictions - using first value since they're the same for all states
  mutate(
    margin_polls = first(as.vector(nat_polls_predictions)),
    margin_fund = first(as.vector(nat_fund_predictions)),
    margin_ensemble = first(as.vector(ensemble_predictions))
  ) %>%
  # Calculate final margins and color categories
  mutate(
    margin_final = partisan_lean + margin_ensemble,
    d_pv = margin_final + 50,
    r_pv = 100 - d_pv,
    category = case_when(
      d_pv > 60 ~ "Strong D",
      d_pv > 55 & d_pv < 60 ~ "Likely D",
      d_pv > 50 & d_pv < 55 ~ "Lean D",
      d_pv > 45 & d_pv < 50 ~ "Lean R",
      d_pv > 40 & d_pv < 45 ~ "Likely R",
      TRUE ~ "Strong R"
    ),
    # Convert color_category to factor with specific ordering
    category = factor(
      category,
      levels = c("Strong R", "Likely R", "Lean R", "Lean D", "Likely D", "Strong D")
    ),
    # calculate electors that each party wins
    d_electors = sum(ifelse(category %in% c("Lean D", "Likely D", "Strong D"), electors, 0)),
    r_electors = sum(ifelse(category %in% c("Lean R", "Likely R", "Strong R"), electors, 0))
  )

electoral_map <- create_electoral_hex_map(df_2024)
electoral_map
```

Clearly, this would be a very unfortunate electoral college result for Vice President Kamala Harris, as she loses almost every single swing state. 


# Quantifying uncertainty
To determine how uncertain my predictions are, we can run Monte Carlo simulations of the election. For the sake of simplicity for this blog post, we will only run the simulations at the state level, and we will assume the national vote margin is true. Our simulations rely on the fact that each state's predicted vote margin is actually a normal random variable, with mean centered at the predicted value and a standard deviation of three percent. (Note: this value is arbitrary and hard-coded, but in future weeks we will find a way of endogenizing it, perhaps by using the square root of the variance in the state's recent voting history as the standard deviation instead.)

Then, following the methodology from the [_Economist_](https://www.economist.com/interactive/us-2024-election/prediction-model/president/how-this-works), we run 10,001 election simulations, recording the total number of electoral college votes each candidate wins in each simulation.

The following graph plots smoothed histograms for the electoral college votes for Harris and Trump respectively

```{r, echo=FALSE, warning=FALSE, message = FALSE}
simulate_election <- function(df_pred, n_simulations, std_dev) {
  # Initialize storage for electoral votes for each simulation
  dem_wins <- numeric(n_simulations)
  rep_wins <- numeric(n_simulations)
  
  for (i in 1:n_simulations) {
    # Simulate partisan lean for each state using a normal distribution
    simulated_partisan_lean <- rnorm(n = nrow(df_pred), mean = df_pred$partisan_lean, sd = std_dev)
    
    # Add the simulated lean to the margin_ensemble to get the final margin
    final_margin <- simulated_partisan_lean + df_pred$margin_ensemble
    
    # Determine who wins each state and aggregate the electoral votes
    dem_wins[i] <- sum(df_pred$electors[final_margin > 0])  # Democratic wins where margin > 0
    rep_wins[i] <- sum(df_pred$electors[final_margin < 0])  # Republican wins where margin < 0
  }
  
  # Calculate win probabilities
  dem_win_prob <- mean(dem_wins >= 270)
  rep_win_prob <- mean(rep_wins >= 270)
  
  # Return a list with the results
  return(list(
    dem_wins = dem_wins,
    rep_wins = rep_wins,
    dem_win_prob = dem_win_prob,
    rep_win_prob = rep_win_prob
  ))
}

simulation_results <- simulate_election(df_2024, 10001, 3)

ggplot() +
  # Smoothed density for Democratic wins
  geom_density(aes(x = simulation_results$dem_wins), fill = "lightblue", color = "blue", alpha = 0.3, adjust = 1.5) +
  # Smoothed density for Republican wins
  geom_density(aes(x = simulation_results$rep_wins), fill = "lightcoral", color = "red", alpha = 0.3, adjust = 1.5) +
  # Vertical line at 270 electoral votes
  geom_vline(xintercept = 270, linetype = "dashed", color = "black", size = 1) +
  # Labels and title
  labs(
    title = "Monte Carlo Simulation Results: Electoral Vote Distribution",
    x = "Electoral Votes",
    y = "Density"
  ) +
  # Clean theme for better visualization
  theme_minimal(base_size = 15) +
  theme(legend.position = "none")

```
From these simulations, Harris wins approximately 1.4 percent of the time, and Trump wins approximately 97.6 percent of the time. (The remaining percent accounts for ties, when both candidates win 269 electoral votes.) Note that the curves plotting Harris's electoral votes and Trump's electoral votes are symmetric. This makes sense, because they must sum to 538.
